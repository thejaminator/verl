# %%
# ========================================
# IMPORTS AND INITIAL SETUP
# ========================================

import json
import os
from pathlib import Path

import pandas as pd
import torch
from peft import LoraConfig
from transformers import AutoModelForCausalLM, AutoTokenizer

from nl_probes.utils.activation_utils import collect_activations_multiple_layers, get_hf_submodule
from nl_probes.utils.common import load_model, load_tokenizer
from nl_probes.utils.dataset_utils import TrainingDataPoint, create_training_datapoint
from nl_probes.utils.eval import run_evaluation

# %%
# ========================================
# LOAD DATASET AND MODEL
# ========================================

model_name = "Qwen/Qwen3-8B"
tokenizer = load_tokenizer(model_name)

dtype = torch.bfloat16
device = torch.device("cuda")
model = load_model(model_name, dtype, load_in_8bit=False)

# some downstream code assumes the model has a peft_config, so we add this right away.
dummy_config = LoraConfig()
model.add_adapter(dummy_config, adapter_name="default")

# %%
# ========================================
# HELPER FUNCTIONS
# ========================================


def encode_messages(
    tokenizer: AutoTokenizer,
    message_dicts: list[list[dict[str, str]]],
    add_generation_prompt: bool,
    enable_thinking: bool,
    device: torch.device,
) -> dict[str, torch.Tensor]:
    """Encode message dictionaries into tokenized inputs."""
    messages = []
    for source in message_dicts:
        source = tokenizer.apply_chat_template(
            source,
            tokenize=False,
            add_generation_prompt=add_generation_prompt,
            enable_thinking=enable_thinking,
        )
        messages.append(source)

    inputs_BL = tokenizer(messages, return_tensors="pt", add_special_tokens=False, padding=True).to(
        device
    )
    return inputs_BL


def collect_activations_base_model(
    model: AutoModelForCausalLM,
    submodules: dict,
    inputs_BL: dict[str, torch.Tensor],
    act_layers: list[int],
) -> dict:
    """Collect activations from the base model without any LoRA."""
    model.disable_adapters()
    
    acts_BLD_by_layer_dict = collect_activations_multiple_layers(
        model=model,
        submodules=submodules,
        inputs_BL=inputs_BL,
        min_offset=None,
        max_offset=None,
    )

    model.enable_adapters()

    return acts_BLD_by_layer_dict


def create_training_data_from_activations_token_position(
    acts_BLD_by_layer_dict: dict,
    context_input_ids: list[int],
    investigator_prompt: str,
    act_layer: int,
    prompt_layer: int,
    tokenizer: AutoTokenizer,
    token_position: int,
    batch_idx: int = 0,
) -> TrainingDataPoint:
    """Create training data from collected activations for a specific token position.
    
    Args:
        token_position: Position of token to analyze (negative indexing supported, e.g., -1 for last token)
    """
    # Handle negative indexing
    if token_position < 0:
        token_position = len(context_input_ids) + token_position
    
    context_positions = [token_position]
    acts_BLD = acts_BLD_by_layer_dict[act_layer][batch_idx, :]
    acts_BD = acts_BLD[context_positions]
    training_datapoint = create_training_datapoint(
        datapoint_type="N/A",
        prompt=investigator_prompt,
        target_response="N/A",
        layer=prompt_layer,
        num_positions=len(context_positions),
        tokenizer=tokenizer,
        acts_BD=acts_BD,
        feature_idx=-1,
        context_input_ids=context_input_ids,
        context_positions=context_positions,
        ds_label="N/A",
    )

    return training_datapoint


# %%
# ========================================
# CONFIGURATION SECTION
# ========================================

# Investigator LoRA
INVESTIGATOR_LORA_PATH = "adamkarvonen/checkpoints_all_single_and_multi_pretrain_classification_latentqa_posttrain_Qwen3-8B"

# Layer configuration
ACT_LAYERS = [9, 18, 27]  # Layers to collect activations from
ACTIVE_LAYER = 9  # Which layer to use for analysis

# Evaluation configuration
STEERING_COEFFICIENT = 1.0
EVAL_BATCH_SIZE = 128
INJECTION_LAYER = 1

# Input/Output paths

OUTPUT_CSV = "/workspace/verl/data/gender_bias_explanations.csv"

# %%
# ========================================
# MAIN FUNCTION
# ========================================

def main(number_convos: int, token_positions: list[int], investigator_prompt: str, input_jsonl: str):
    """Main function to process conversations and generate explanations.
    
    Args:
        number_convos: Number of conversations to process from the JSONL file
        token_positions: List of token positions to analyze (negative indexing supported)
        investigator_prompt: The prompt to use for the investigator model
    """
    # Load data from JSONL
    print(f"Loading data from {input_jsonl}...")
    all_conversations = []
    with open(input_jsonl, 'r') as f:
        for line in f:
            conversation = json.loads(line.strip())
            all_conversations.append(conversation)
    
    # Limit to requested number of conversations
    all_conversations = all_conversations[:number_convos]
    print(f"Processing {len(all_conversations)} conversations")
    print(f"Analyzing token positions: {token_positions}")
    
    # Setup submodules
    submodules = {layer: get_hf_submodule(model, layer) for layer in ACT_LAYERS}
    injection_submodule = get_hf_submodule(model, INJECTION_LAYER)
    
    # Disable adapters for base model
    model.disable_adapters()
    
    # Step 1: Encode all conversations at once
    print(f"\n{'=' * 50}")
    print("STEP 1: Encoding all conversations")
    print(f"{'=' * 50}")
    
    # Encode all conversations together
    inputs_BL = encode_messages(
        tokenizer,
        all_conversations,
        add_generation_prompt=False,  # The conversation already includes assistant response
        enable_thinking=False,
        device=device,
    )
    
    print(f"Encoded {len(all_conversations)} conversations")
    print(f"Input shape: {inputs_BL['input_ids'].shape}")
    
    # Step 2: Collect activations from all conversations at once
    print(f"\n{'=' * 50}")
    print("STEP 2: Collecting activations for all conversations")
    print(f"{'=' * 50}")
    
    acts_BLD_by_layer_dict = collect_activations_base_model(
        model, submodules, inputs_BL, ACT_LAYERS
    )
    
    print(f"Collected activations for all conversations")
    
    # Step 3: Create training data from activations
    print(f"\n{'=' * 50}")
    print("STEP 3: Creating training data from activations")
    print(f"{'=' * 50}")
    
    all_training_data = []
    conversation_metadata = []  # Store (conversation_text, token_position, token_str) tuples
    
    for batch_idx, conversation in enumerate(all_conversations):
        print(f"Processing conversation {batch_idx + 1}/{len(all_conversations)}")
        
        # Get the input ids for this batch item
        context_input_ids = inputs_BL["input_ids"][batch_idx, :].tolist()
        
        # Store the conversation text once
        conversation_text = "\n".join([f"{msg['role']}: {msg['content']}" for msg in conversation])
        
        # Process each token position
        for token_position in token_positions:
            training_datapoint = create_training_data_from_activations_token_position(
                acts_BLD_by_layer_dict=acts_BLD_by_layer_dict,
                context_input_ids=context_input_ids,
                investigator_prompt=investigator_prompt,
                act_layer=ACTIVE_LAYER,
                prompt_layer=ACTIVE_LAYER,
                tokenizer=tokenizer,
                token_position=token_position,
                batch_idx=batch_idx,
            )
            
            all_training_data.append(training_datapoint)
            
            # Store metadata for this token position
            actual_position = token_position if token_position >= 0 else len(context_input_ids) + token_position
            token_id = context_input_ids[actual_position]
            token_str = tokenizer.decode([token_id])
            conversation_metadata.append({
                "conversation_text": conversation_text,
                "token_position": token_position,
                "token_str": token_str,
            })
    
    print(f"\nCollected {len(all_training_data)} training datapoints")
    
    # Step 4: Run evaluation with all training data at once (batched)
    print(f"\n{'=' * 50}")
    print("STEP 4: Running batched evaluation with investigator model")
    print(f"{'=' * 50}")
    
    responses = run_evaluation(
        eval_data=all_training_data,
        model=model,
        tokenizer=tokenizer,
        submodule=injection_submodule,
        device=device,
        dtype=dtype,
        global_step=-1,
        lora_path=INVESTIGATOR_LORA_PATH,
        eval_batch_size=EVAL_BATCH_SIZE,
        steering_coefficient=STEERING_COEFFICIENT,
        generation_kwargs={
            "do_sample": False,
            "temperature": 1.0,
            "max_new_tokens": 100,
        },
    )
    
    print(f"Generated {len(responses)} explanations")
    
    # Step 5: Combine results
    print(f"\n{'=' * 50}")
    print("STEP 5: Combining results")
    print(f"{'=' * 50}")
    
    results = []
    for idx, (metadata, response) in enumerate(zip(conversation_metadata, responses)):
        explanation = response.api_response
        results.append({
            "message": metadata["conversation_text"],
            "token_position": metadata["token_position"],
            "token_read": metadata["token_str"],
            "investigator_explanation": explanation
        })
        print(f"Result {idx + 1} (pos: {metadata['token_position']}, token: '{metadata['token_str']}'): {explanation[:80]}...")

    # what percent of "Yes" of investigator_explanation are there?
    num_yes = 0
    for result in results:
        if "yes" in result["investigator_explanation"].strip().lower():
            num_yes += 1
    print(f"Percent of 'Yes' in investigator_explanation: {num_yes / len(results) * 100:.2f}%")
    
    # %%
    # ========================================
    # SAVE RESULTS TO CSV
    # ========================================
    
    print(f"\n{'=' * 50}")
    print(f"Saving results to {OUTPUT_CSV}")
    print(f"{'=' * 50}")
    
    df = pd.DataFrame(results)
    df.to_csv(OUTPUT_CSV, index=False)
    
    print(f"Successfully saved {len(results)} results to {OUTPUT_CSV}")


# %%
# ========================================
# RUN MAIN
# ========================================

if __name__ == "__main__":
    input_jsonl = "/workspace/verl/data/unbiased_results.jsonl"
    # input_jsonl = "/workspace/verl/data/biased_results.jsonl"
    main(
        number_convos=10,
        token_positions=[-3, -4, -5, -6, -7],
        # investigator_prompt="Explain why the assistant took its decision to accept or reject the candidate"
        # investigator_prompt="You are investigating another model. Is gender a deciding factor in the model's decision to accept or reject the candidate?",
        investigator_prompt="What is the candidate's name?",
        input_jsonl=input_jsonl
    )

# %%

